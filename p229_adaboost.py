'''
Created on Jul 10, 2016
adaboost.py

Adaboost builds a classifier by starting with weak learners like a forest 
decision tree, selecting training set samples without replacement, training
a stump, finding samples that are in error, adding a decision tree stump,
to train those weak samples, updating weights to be applied to the samples for 
computing the final prediction.

It increasing emphasizes the weights of outlier samples until they are result in
a sequence of weights and decision trees that handle those samples.

from Python Machine Learning by Sebastian Raschka

@author: richard lyman
'''

from sklearn.ensemble import AdaBoostClassifier
      
import ocr_utils
from sklearn.cross_validation import train_test_split
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder

charsToTrain=(48,51)
nChars = 1000
y, X, y_test, X_test, labels  = ocr_utils.load_E13B(chars_to_train = charsToTrain , columns=(9,17), nChars=nChars)

le = LabelEncoder()
y = le.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.40,random_state=1)


from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
tree = DecisionTreeClassifier(criterion='entropy', 
                              max_depth=1)

ada = AdaBoostClassifier(base_estimator=tree,
                         n_estimators=500, 
                         learning_rate=0.1,
                         random_state=0)

tree = tree.fit(X_train, y_train)
y_train_pred = tree.predict(X_train)
y_test_pred = tree.predict(X_test)

tree_train = accuracy_score(y_train, y_train_pred)
tree_test = accuracy_score(y_test, y_test_pred)
print('Decision tree train/test accuracies %.3f/%.3f'
      % (tree_train, tree_test))

ada = ada.fit(X_train, y_train)
y_train_pred = ada.predict(X_train)
y_test_pred = ada.predict(X_test)

ada_train = accuracy_score(y_train, y_train_pred) 
ada_test = accuracy_score(y_test, y_test_pred) 
print('AdaBoost train/test accuracies %.3f/%.3f'
      % (ada_train, ada_test))



x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1
y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, (x_max-x_min)/100),
                     np.arange(y_min, y_max, (y_max-y_min)/100))

f, axarr = plt.subplots(1, 2, sharex='col', sharey='row', figsize=(8, 3))

title='AdaBoost'
for idx, clf, tt in zip([0, 1],
                        [tree, ada],
                        ['Decision Tree', title]):
    clf.fit(X_train, y_train)
    
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    axarr[idx].contourf(xx, yy, Z, alpha=0.3)
    axarr[idx].scatter(X_train[y_train==0, 0], 
                       X_train[y_train==0, 1], 
                       c='blue', marker='^')
    axarr[idx].scatter(X_train[y_train==1, 0], 
                       X_train[y_train==1, 1], 
                       c='red', marker='o')
    axarr[idx].set_title(tt)
    axarr[idx].set_ylabel(labels[0], fontsize=12)
    axarr[idx].set_xlabel(labels[1], fontsize=12)

plt.tight_layout()

ocr_utils.show_figures(plt, title)

print ('\n########################### No Errors ####################################')
